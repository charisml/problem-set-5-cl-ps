---
title: "PS5"
author: "Charisma Lambert and Prashanthi Subbiah"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Charisma Lambert (charisml)
    - Partner 2 (name and cnet ID): Prashanthi Subbiah ()
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*CL\*\* \*\*PS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = 'https://oig.hhs.gov/fraud/enforcement/' 
response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

#print(response)
#print(soup)

main_nochildren = soup.find_all(lambda t: t.name == 'main' and not
t.find_all())

# Find all <h2> tags within <main>
h2_tags = soup.find_all('h2')

# Extract links from <a> tags within each <h2>
links = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links.append(a_tag['href'])

# Print the extracted links
#print(links)

# Extracting titles of offences corresponding to each link
links_text = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links_text.append(a_tag.text)

#print(links_text) - commented out after checking

# Extracting Dates
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <span> tags within each <div> inside <header>
span_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        span_tag = div.find('span')
        if span_tag:
            span_texts.append(span_tag.text.strip())

# Print the extracted text
filtered_texts = span_texts[4:] 

#print(filtered_texts) - commented out after checking

# Filtering for Categories
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <li> tags within each <ul> inside <div> within <header>
li_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        ul_tags = div.find_all('ul')
        for ul in ul_tags:
            li_tags = ul.find_all('li')
            for li in li_tags:
                li_texts.append(li.text.strip())

# Assuming you have a list called `li_texts`
# Get the last 20 elements
last_20_texts = li_texts[-20:]

#print(last_20_texts) - commented out after checking

# Create a DataFrame from the lists
df = pd.DataFrame({
    'Title': links_text,
    'Date': filtered_texts,
    'Category': last_20_texts,
    'Link': links
})

# Print the DataFrame
print(df.head(10))

# BingChat Query: How do I filter for an <a> tag in an <h2> tag?
```

  
### 2. Crawling (PARTNER 1)

```{python}
base_url = 'https://oig.hhs.gov'

# This extracts all text from the second <li> tag
agency_info_list = [] # Add your list of URLs here
for i in df['Link']:
    # Specify the URL
    url = base_url + i
    
    # Fetch the page content
    response = requests.get(url)
    response.raise_for_status()
    
    # Parse the HTML content with Beautiful Soup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Use the CSS selector path to find the element
    agency_info = soup.select_one('#main-content > div > div:nth-child(2) > article > div > ul > li:nth-child(2)')
    
    # Check if the element is found and print its text
    if agency_info: 
        agency_info_list.append(agency_info.get_text(strip=True)) 
    else: 
        agency_info_list.append("Agency information not found.")
    
# Taking out word "Agency:"
cleaned_agency_info_list = [info.replace("Agency:", "").strip() for info in agency_info_list] 

# Appending 'Enforcement Agency' column to df
df['Enforcement Agency'] = cleaned_agency_info_list 
print(df.head(10))

# BingChat Query: How do I filter for a <li> tag, which has a nested <span> tag that has the text "Agency:" in it?

# Here is the XPath: '#main-content > div > div:nth-child(2) > article > div > ul > li:nth-child(2)'
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
import time 
from datetime import datetime

function(int(month), int(year)):
  if year < 2013: 
    print("The date is invalid. Please retry with a year that is 2013 or later.")
    return
  
  base_url = " link  "
  title = []
  category = []
  date = []
  enforcement_agency = []

  start_date = f"{year}-{month}-01"
  end_date = datetime.today()

  page = 1
  while True:
    url = f"{base_url}?page={page}""
    for blank in df:
      scrape page using partnern section

      start_date_month + 1, start_date_year+1 and <=  end_date
      time.sleep(1)
  df = pd.DataFrame({ build dataframe})
  df.to_csv("enforcement_actions_year_month")

  return df

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from datetime import datetime
import pandas as pd
import time
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
import requests
from bs4 import BeautifulSoup


def get_enforcement_actions(year, month):
    if year < 2013:
        print("Please input a year >= 2013. Only enforcement actions after 2013 are listed.")
        return None
    
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    page = 1  # Start from page 1
    
    titles, dates, categories, links, agencies = [], [], [], [], []
    target_date = datetime(year, month, 1).date()
    
    while True:
        # Construct the URL dynamically based on the current page
        if page == 1:
            current_url = base_url  # First page doesn't have ?page=1
        else:
            current_url = f"{base_url}?page={page}"

        #print(f"Currently scraping: {current_url}") - commented out 
        response = requests.get(current_url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Find all enforcement action cards on the current page
        li_us_card = soup.find_all("li", class_="usa-card card--list pep-card--minimal mobile:grid-col-12")
        
        # If no cards are found on the page, stop scraping (end of pages)
        if not li_us_card:
            print(f"No more entries found on page {page}. Stopping.")
            break
        
        # Scrape data from each card
        for li in li_us_card:
            date_str = li.find(class_="text-base-dark padding-right-105").get_text(strip=True)
            entry_date = datetime.strptime(date_str, "%B %d, %Y").date()
            
            if entry_date >= target_date:
                title = li.find("h2", class_="usa-card__heading").find("a").get_text(strip=True)
                category = li.find("ul", class_="display-inline add-list-reset").find("li").get_text(strip=True)
                link = base_url + li.find("a")["href"]
                
                titles.append(title)
                dates.append(date_str)
                categories.append(category)
                links.append(link)
                
                # Crawl individual action page for agency
                action_response = requests.get(link)
                action_soup = BeautifulSoup(action_response.text, "html.parser")
                agency_ul = action_soup.find("ul", class_="usa-list usa-list--unstyled margin-y-2")
                if agency_ul and len(agency_ul.find_all("li")) > 1:
                    agency = agency_ul.find_all("li")[1].get_text(strip=True).replace("Agency:", "").strip()
                else:
                    agency = "N/A"
                agencies.append(agency)
            else:
                # If we've reached an entry older than the target date, stop scraping
                return pd.DataFrame({
                    "Title of Enforcement Action": titles,
                    "Date": dates,
                    "Category": categories,
                    "Link": links,
                    "Agency": agencies
                })
        
        # Increment the page counter to move to the next page
        page += 1
        
        # Add a delay between requests to avoid overloading the server
        time.sleep(0.2)

    # If we've gone through all pages without finding an older entry
    return pd.DataFrame({
        "Title of Enforcement Action": titles,
        "Date": dates,
        "Category": categories,
        "Link": links,
        "Agency": agencies
    })

# Citation: Date scrapper was now including other aspects of the page with the same encoding so I created an if/else that continued to have errors. I entered the if/else into ChatGPT and it suggested try-except. Logic within is my own, just replaced if/else with try and except.

# Citation:  Searched how to turn string date into datetime object with YYYY-MM-DD format and found stackoverflow thread that used the syntax used in this section for date conversion. 

```

I got X enforcement actions in my dataframe. The earliest enforcement action is ______ on _______ day.

Creating Dataframe for January 2023 onwards

```{python}
result_df_2023 = get_enforcement_actions(2023, 1)
print(result_df_2023.head(5))
```

* c. Test Partner's Code (PARTNER 1)

Creating Dataframe for January 2021 onwards
```{python}
result_df_2021 = scrape_enforcement_actions(2021, 1)
print(result_df_2021.head(5))
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```