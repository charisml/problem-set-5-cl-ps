---
title: "PS5"
author: "Charisma Lambert and Prashanthi Subbiah"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Charisma Lambert (charisml)
    - Partner 2 (name and cnet ID): Prashanthi Subbiah ()
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*CL\*\* \*\*PS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = 'https://oig.hhs.gov/fraud/enforcement/' 
response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

#print(response)
#print(soup)

main_nochildren = soup.find_all(lambda t: t.name == 'main' and not
t.find_all())

# Find all <h2> tags within <main>
h2_tags = soup.find_all('h2')

# Extract links from <a> tags within each <h2>
links = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links.append(a_tag['href'])

# Print the extracted links
#print(links)

# Extracting titles of offences corresponding to each link
links_text = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links_text.append(a_tag.text)

#print(links_text)

# Extracting Dates
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <span> tags within each <div> inside <header>
span_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        span_tag = div.find('span')
        if span_tag:
            span_texts.append(span_tag.text.strip())

# Print the extracted text
#for text in span_texts:
    #print(text)

filtered_texts = span_texts[4:] 

#print(filtered_texts)

# Filtering for Categories
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <li> tags within each <ul> inside <div> within <header>
li_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        ul_tags = div.find_all('ul')
        for ul in ul_tags:
            li_tags = ul.find_all('li')
            for li in li_tags:
                li_texts.append(li.text.strip())

# Assuming you have a list called `li_texts`
# Get the last 20 elements
last_20_texts = li_texts[-20:]

#print(last_20_texts)

# Create a DataFrame from the lists
df = pd.DataFrame({
    'Title': links_text,
    'Date': filtered_texts,
    'Category': last_20_texts,
    'Link': links
})

# Print the DataFrame
df.head(10)
```

  
### 2. Crawling (PARTNER 1)

```{python}
base_url = 'https://oig.hhs.gov'

# This extracts all text from the second <li> tag
agency_info_list = [] # Add your list of URLs here
for i in df['Link']:
    # Specify the URL
    url = base_url + i
    
    # Fetch the page content
    response = requests.get(url)
    response.raise_for_status()
    
    # Parse the HTML content with Beautiful Soup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Use the CSS selector path to find the element
    agency_info = soup.select_one('#main-content > div > div:nth-child(2) > article > div > ul > li:nth-child(2)')
    
    # Check if the element is found and print its text
    if agency_info: 
        agency_info_list.append(agency_info.get_text(strip=True)) 
    else: 
        agency_info_list.append("Agency information not found.")
    
# Taking out word "Agency:"
cleaned_agency_info_list = [info.replace("Agency:", "").strip() for info in agency_info_list] 

# Appending 'Enforcement Agency' column to df
df['Enforcement Agency'] = cleaned_agency_info_list 
df
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
import time 
from datetime import datetime

function(int(month), int(year)):
  if year < 2013: 
    print("The date is invalid. Please retry with a year that is 2013 or later.")
    return
  
  base_url = " link  "
  title = []
  category = []
  date = []
  enforcement_agency = []

  start_date = f"{year}-{month}-01"
  end_date = datetime.today()

  page = 1
  while True:
    url = f"{base_url}?page={page}""
    for blank in df:
      scrape page using partnern section

      start_date_month + 1, start_date_year+1 and <=  end_date
      time.sleep(1)
  df = pd.DataFrame({ build dataframe})
  df.to_csv("enforcement_actions_year_month")

  return df

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from datetime import datetime 

def get_enforcement_actions(month, year):
  if year < 2013:
    print("The date is invalid. Please retry with a year that is 2013 or later.")
    return

  base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
  page = 1

  current_date = datetime.today()
  start_date = datetime(year, month, 1)

  title = []
  dates = []
  links = []
  categories = []

  page = 1
  while True:
    url = base_url + f"{page}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'lxml')

    main_nochildren = soup.find_all(lambda t: t.name == 'main' and not t.find_all())

    h2_tags = soup.find_all('h2')

    # Extract links from <a> tags within each <h2>
    for h2 in h2_tags:
      a_tag = h2.find('a', href=True)
      if a_tag:
        links.append(a_tag['href'])

    for h2 in h2_tags:
      a_tag = h2.find('a', href=True)
      if a_tag:
        title.append(a_tag.text)

    for header in header_tags:
      div_tags = header.find_all('div')
      for div in div_tags:
        span_tag = div.find('span')
        if span_tag:
            dates.append(span_tag.text.strip())

    updated_dates = []
    for date in dates:
      try:
        date = datetime.strptime(date, "%B %d, %Y").strftime
        updated_dates.append(date)
      except ValueError:
        pass

    filtered_texts = updated_dates[4:] 


    for header in header_tags:
      div_tags = header.find_all('div')
      for div in div_tags:
        ul_tags = div.find_all('ul')
        for ul in ul_tags:
          li_tags = ul.find_all('li')
          for li in li_tags:
            categories.append(li.text.strip())

    last_20_texts = categories[-20:]
    time.sleep(1)

    if start_date.month == 12:
      start_date = datetime(start_date.year + 1, 1, 1)
    else: 
      start_date = datetime(start_date.year, start_date.month + 1, 1)

    page += 1


  df = pd.DataFrame({
    'Title': title,
    'Date': filtered_texts,
    'Category': last_20_texts,
    'Link': links
  })

  csv_name = f"enforcement_actions_{year}_{month}"
  df.to_csv(csv_name)

  return df
  
get_enforcement_actions(1, 2023)

# Citation: Date scrapper was now including other aspects of the page with the same encoding so I created an if/else that continued to have errors. I entered the if/else into ChatGPT and it suggested try-except. Logic within is my own, just replaced if/else with try and except.

# Citation:  Searched how to turn string date into datetime object with YYYY-MM-DD format and found stackoverflow thread that used the syntax used in this section for date conversion. 

```

I got X enforcement actions in my dataframe. The earliest enforcement action is ______ on _______ day.

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```