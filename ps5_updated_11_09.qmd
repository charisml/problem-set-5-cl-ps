---
title: "PS5"
author: "Charisma Lambert and Prashanthi Subbiah"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Charisma Lambert (charisml)
    - Partner 2 (name and cnet ID): Prashanthi Subbiah ()
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*CL\*\* \*\*PS\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup

url = 'https://oig.hhs.gov/fraud/enforcement/' 
response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml')

#print(response)
#print(soup)

main_nochildren = soup.find_all(lambda t: t.name == 'main' and not
t.find_all())

# Find all <h2> tags within <main>
h2_tags = soup.find_all('h2')

# Extract links from <a> tags within each <h2>
links = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links.append(a_tag['href'])

# Print the extracted links
#print(links)

# Extracting titles of offences corresponding to each link
links_text = []
for h2 in h2_tags:
    a_tag = h2.find('a', href=True)
    if a_tag:
        links_text.append(a_tag.text)

#print(links_text) - commented out after checking

# Extracting Dates
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <span> tags within each <div> inside <header>
span_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        span_tag = div.find('span')
        if span_tag:
            span_texts.append(span_tag.text.strip())

# Print the extracted text
filtered_texts = span_texts[4:] 

#print(filtered_texts) - commented out after checking

# Filtering for Categories
# Find all <header> elements
header_tags = soup.find_all('header')

# Extract text from <li> tags within each <ul> inside <div> within <header>
li_texts = []
for header in header_tags:
    div_tags = header.find_all('div')
    for div in div_tags:
        ul_tags = div.find_all('ul')
        for ul in ul_tags:
            li_tags = ul.find_all('li')
            for li in li_tags:
                li_texts.append(li.text.strip())

# Assuming you have a list called `li_texts`
# Get the last 20 elements
last_20_texts = li_texts[-20:]

#print(last_20_texts) - commented out after checking

# Create a DataFrame from the lists
df = pd.DataFrame({
    'Title': links_text,
    'Date': filtered_texts,
    'Category': last_20_texts,
    'Link': links
})

# Print the DataFrame
print(df.head(10))

# BingChat Query: How do I filter for an <a> tag in an <h2> tag?
```

  
### 2. Crawling (PARTNER 1)

```{python}
base_url = 'https://oig.hhs.gov'

# This extracts all text from the second <li> tag
agency_info_list = [] # Add your list of URLs here
for i in df['Link']:
    # Specify the URL
    url = base_url + i
    
    # Fetch the page content
    response = requests.get(url)
    response.raise_for_status()
    
    # Parse the HTML content with Beautiful Soup
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Use the CSS selector path to find the element
    agency_info = soup.select_one('#main-content > div > div:nth-child(2) > article > div > ul > li:nth-child(2)')
    
    # Check if the element is found and print its text
    if agency_info: 
        agency_info_list.append(agency_info.get_text(strip=True)) 
    else: 
        agency_info_list.append("Agency information not found.")
    
# Taking out word "Agency:"
cleaned_agency_info_list = [info.replace("Agency:", "").strip() for info in agency_info_list] 

# Appending 'Enforcement Agency' column to df
df['Enforcement Agency'] = cleaned_agency_info_list 
print(df.head(10))

# BingChat Query: How do I filter for a <li> tag, which has a nested <span> tag that has the text "Agency:" in it?

# Here is the XPath: '#main-content > div > div:nth-child(2) > article > div > ul > li:nth-child(2)'
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
import time 
from datetime import datetime

function(int(month), int(year)):
  if year < 2013: 
    print("The date is invalid. Please retry with a year that is 2013 or later.")
    return
  
  base_url = " link  "
  title = []
  category = []
  date = []
  enforcement_agency = []

  start_date = f"{year}-{month}-01"
  end_date = datetime.today()

  page = 1
  while True:
    url = f"{base_url}?page={page}""
    for blank in df:
      scrape page using partnern section

      start_date_month + 1, start_date_year+1 and <=  end_date
      time.sleep(1)
  df = pd.DataFrame({ build dataframe})
  df.to_csv("enforcement_actions_year_month")

  return df

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from datetime import datetime
import pandas as pd
import time
import altair as alt
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
import requests
from bs4 import BeautifulSoup


def get_enforcement_actions(year, month):
    if year < 2013:
        print("Please input a year >= 2013. Only enforcement actions after 2013 are listed.")
        return None
    
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    page = 1  # Start from page 1
    
    titles, dates, categories, links, agencies = [], [], [], [], []
    target_date = datetime(year, month, 1).date()
    
    while True:
        # Construct the URL dynamically based on the current page
        if page == 1:
            current_url = base_url  # First page doesn't have ?page=1
        else:
            current_url = f"{base_url}?page={page}"

        print(f"Currently scraping: {current_url}") 
        response = requests.get(current_url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Find all enforcement action cards on the current page
        li_us_card = soup.find_all("li", class_="usa-card card--list pep-card--minimal mobile:grid-col-12")
        
        # If no cards are found on the page, stop scraping (end of pages)
        if not li_us_card:
            print(f"No more entries found on page {page}. Stopping.")
            break
        
        # Scrape data from each card
        for li in li_us_card:
            date_str = li.find(class_="text-base-dark padding-right-105").get_text(strip=True)
            entry_date = datetime.strptime(date_str, "%B %d, %Y").date()
            
            if entry_date >= target_date:
                title = li.find("h2", class_="usa-card__heading").find("a").get_text(strip=True)
                category = li.find("ul", class_="display-inline add-list-reset").find("li").get_text(strip=True)
                link = base_url + li.find("a")["href"]
                
                titles.append(title)
                dates.append(date_str)
                categories.append(category)
                links.append(link)
                
                # Crawl individual action page for agency
                action_response = requests.get(link)
                action_soup = BeautifulSoup(action_response.text, "html.parser")
                agency_ul = action_soup.find("ul", class_="usa-list usa-list--unstyled margin-y-2")
                if agency_ul and len(agency_ul.find_all("li")) > 1:
                    agency = agency_ul.find_all("li")[1].get_text(strip=True).replace("Agency:", "").strip()
                else:
                    agency = "N/A"
                agencies.append(agency)
            else:
                # If we've reached an entry older than the target date, stop scraping
                return pd.DataFrame({
                    "Title of Enforcement Action": titles,
                    "Date": dates,
                    "Category": categories,
                    "Link": links,
                    "Agency": agencies
                })
        
        # Increment the page counter to move to the next page
        page += 1
        
        # Add a delay between requests to avoid overloading the server
        time.sleep(0.2)

    # If we've gone through all pages without finding an older entry
    return pd.DataFrame({
        "Title of Enforcement Action": titles,
        "Date": dates,
        "Category": categories,
        "Link": links,
        "Agency": agencies
    })

# Citation: Date scrapper was now including other aspects of the page with the same encoding so I created an if/else that continued to have errors. I entered the if/else into ChatGPT and it suggested try-except. Logic within is my own, just replaced if/else with try and except.

# Citation:  Searched how to turn string date into datetime object with YYYY-MM-DD format and found stackoverflow thread that used the syntax used in this section for date conversion. 

```

I got X enforcement actions in my dataframe. The earliest enforcement action is ______ on _______ day.

Creating Dataframe for January 2023 onwards

```{python}
result_df_2023 = get_enforcement_actions(2023, 1)
print(result_df_2023.head(5))
```

* c. Test Partner's Code (PARTNER 1)

Creating Dataframe for January 2021 onwards
```{python}
result_df_2021 = get_enforcement_actions(2021, 1)
print(result_df_2021.head(5))
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
result_df_2021
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
import pandas as pd 
import altair as alt
import numpy as np

# Path to Excel file
file_path = "C:/Users/prash/OneDrive/Documents/Python II/PSet 5/output.xlsx"

# Read the Excel file into a DataFrame
result_df_2021 = pd.read_excel(file_path)

# Converting date from "January 1, 2021" to "01-01-2021"
result_df_2021['Date'] = pd.to_datetime(result_df_2021['Date'], format='%B %d, %Y') 

# Extract month and year and combine them into a single column 
result_df_2021['Month_Year'] = result_df_2021['Date'].dt.to_period('M').astype(str) 

# Define the conditions
conditions = [
    result_df_2021['Category'] == 'State Enforcement Agencies',
    result_df_2021['Category'] == 'Criminal and Civil Actions'
]

# Define the corresponding choices
choices = [
    'State Enforcement Agencies',
    'Criminal and Civil Actions'
]

# Use np.select to create the new 'Category_New' column with three categories
result_df_2021['Category_New'] = np.select(conditions, choices, default='Other')

# Check the resulting DataFrame
result_df_2021


# Dataframe grouping by Month_Year and Category to summarize Number of Enforcement Actions
count_2021_category = result_df_2021.groupby(['Category_New', 'Month_Year']).agg(Count =('Title of Enforcement Action', 'count')).reset_index()

# Filter out the "Other" category
filtered_df = count_2021_category[count_2021_category['Category_New'] != 'Other']

# Create the Altair chart
line_chart = alt.Chart(filtered_df).mark_line(point=True).encode(
    x=alt.X('Month_Year:O', title='Time (Year-Month)'),
    y=alt.Y('Count:Q', title='Number of Actions'),
    color=alt.Color('Category_New:N', title='Category')
).properties(
    title='Number of Enforcement Actions by Category (January 2021 onwards)',
    width=800,
    height=400
)

# Display the chart
line_chart.display()
```

* based on five topics

```{python}
import pandas as pd 
import altair as alt
import numpy as np

# Path to your Excel file
file_path = "C:/Users/prash/OneDrive/Documents/Python II/PSet 5/output.xlsx"

# Read the Excel file into a DataFrame
result_df_2021 = pd.read_excel(file_path)

# Converting date from "January 1, 2021" to "01-01-2021"
result_df_2021['Date'] = pd.to_datetime(result_df_2021['Date'], format='%B %d, %Y') 

# Extract month and year and combine them into a single column 
result_df_2021['Month_Year'] = result_df_2021['Date'].dt.to_period('M').astype(str)

# Define the conditions
conditions = [
    result_df_2021['Category'] == 'State Enforcement Agencies',
    result_df_2021['Category'] == 'Criminal and Civil Actions'
]

# Define the corresponding choices
choices = [
    'State Enforcement Agencies',
    'Criminal and Civil Actions'
]

# Use np.select to create the new 'Category_New' column with three categories
result_df_2021['Category_New'] = np.select(conditions, choices, default='Other')

# Filtering for only ['Category_New'] == "Criminal and Civil Actions", so that we can further classify 
df_2021_civil_mask = result_df_2021['Category_New'] == "Criminal and Civil Actions"
df_2021_civil = result_df_2021[df_2021_civil_mask] # Queried to help correct it

# Sub-categorizing "Criminal and Civil Actions" into “Health Care Fraud”, “Financial Fraud”, “Drug Enforcement”, “Bribery/Corruption”, and “Other” - new colum names "Topic"
topics = { 'Health Care Fraud': ['health', 'care', 'pharmacist', 'doctor', 'medic'], 'Financial Fraud': ['financial', 'bank', 'scam'], 'Drug Enforcement': ['drug', 'raid', 'narcotics'], 'Bribery/Corruption': ['bribe', 'corruption', 'charges', 'dollars'], 'Other': [] } 

# Function to assign a topic to each action - queried in BingChat on how to assign a topic to each enforcement action
def assign_topic(title): 
    for topic, keywords in topics.items(): 
        if any(keyword in title.lower() for keyword in keywords): 
            return topic 
    return 'Other' 

# Using function on "Criminal and Civil Actions"
df_2021_civil['Topic'] = df_2021_civil.apply(lambda row: assign_topic(row['Title of Enforcement Action']), axis=1)

# Filtering for only ['Category_New'] == "State Enforcement Agencies" (not "Criminal and Civil Actions")
df_2021_state_mask = result_df_2021['Category_New'] == "State Enforcement Agencies"
df_2021_state = result_df_2021[df_2021_state_mask]
df_2021_state['Topic'] = df_2021_state['Category_New']

df_2021_other_mask = result_df_2021['Category_New'] == "Other"
df_2021_other = result_df_2021[df_2021_other_mask]
df_2021_other['Topic'] = "Misc non-Criminal and Civil Actions"

# Combining data for ['Category_New'] == "State Enforcement Agencies" with sub-classified ['Category_New'] == "Criminal and Civil Actions"
combined_df_2021 = pd.concat([df_2021_civil, df_2021_state, df_2021_other], ignore_index= True)
combined_df_2021['Topic'].unique()

# Dataframe grouping by Month_Year and Topic to summarize Number of Enforcement Actions 
summary_df_2021 = combined_df_2021.groupby(['Topic', 'Month_Year']).agg(Count =('Title of Enforcement Action', 'count')).reset_index()

# Create the Altair chart
line_chart_new = alt.Chart(summary_df_2021).mark_line(point=True).encode(
    x=alt.X('Month_Year:O', title='Time(Year-Month)'),
    y=alt.Y('Count:Q', title='Number of Actions'),
    color=alt.Color('Topic:N', title='Category')
).properties(
    title='Number of Enforcement Actions by Category with Sub-Categories of Criminal and Civil Actions (January 2021 onwards)',
    width=800,
    height=400
)

# Display the chart
line_chart_new.display()
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import pandas as pd
import warnings
import geopandas as gdp
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")

zip_filepath = "C:/Users/prash/Downloads/cb_2018_us_state_500k (1)/cb_2018_us_state_500k.shp"
zip_file = gdp.read_file(zip_filepath)
zip_file # NAME just has name of the state

# Create new column called name from Agency that just has the name of the state ("NAME")
combined_df_2021['NAME'] = combined_df_2021['Agency_Info'].apply(lambda x: x.split('State of ')[1].split(' ')[0])

# Grouping Number of Actions by State ("NAME")
actions_by_state = combined_df_2021.groupby(['NAME']).agg(Count =('Title of Enforcement Action', 'count')).reset_index()
actions_by_state.columns = ["NAME", "Count"]

# Merging dataframes
actions_by_state_merge = zip_file.merge(actions_by_state, left_on = "NAME", right_on = "NAME", how = "left")

# Making sure encoding is right
actions_by_state_merge = actions_by_state_merge.to_crs("EPSG:5070")

# Plotting Choropleth of hospitals by count of actions in each state
fig, ax = plt.subplots(1, 1, figsize=(15, 10)) 
actions_by_state_merge.plot(column='Count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True) # color scheme and formatting aided by BingChat
ax.set_title('Number of Enforcement Actions by State', fontsize=15) 
ax.set_axis_off() 
plt.show()
```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```